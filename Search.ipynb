{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Search.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "5PXu8fp9ua79",
        "colab_type": "code",
        "outputId": "777b1cd7-00de-4b79-ea04-28271063ed0b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "twsFeszHvTC9",
        "colab_type": "code",
        "outputId": "c8230725-1a08-4aa5-a835-24315ad2526a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "import pandas as pd\n",
        "import base64\n",
        "import numpy as np\n",
        "from math import log\n",
        "import os\n",
        "import scipy\n",
        "import gensim\n",
        "import re\n",
        "from copy import deepcopy\n",
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk import word_tokenize\n",
        "import gensim.corpora as corpora\n",
        "import itertools\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "from nltk.corpus import stopwords\n",
        "stops = stopwords.words('english')\n",
        "\n",
        "''' This method removes all kinds of line breaks. '''\n",
        "def removeLineBreaks(tweet):\n",
        "    return re.sub(\"\\n\\r|\\r\\n|\\n|\\r\",\" \", tweet)\n",
        "\n",
        "''' This method removes all the url's in the tweet'''\n",
        "def removeURLs(tweet):\n",
        "    return re.sub(\"(http|ftp|https)://([\\w_-]+(?:(?:\\.[\\w_-]+)+))([\\w.,@?^=%&:/~+#-]*[\\w@?^=%&/~+#-])?\", \" \", tweet)\n",
        "\n",
        "''' This method removes all emojis from the tweet'''\n",
        "def removeEmojis(tweet):\n",
        "    tweet = tweet.encode('ascii', 'ignore').decode('ascii')\n",
        "    return tweet\n",
        "\n",
        "''' This method checks if the tweet is a retweet or not.\n",
        "    a retweet contains RT @***** '''\n",
        "def isRetweet(tweet):\n",
        "    retweet = re.compile(\"RT @[A-Za-z0-9]*:\")\n",
        "    retweet.match(tweet)\n",
        "\n",
        "    return bool(re.search(\"RT @[A-Za-z0-9]*:\", tweet))\n",
        "\n",
        "''' This method removes the retweet tag from tweets'''\n",
        "def removeRTtag(tweet):\n",
        "    return re.sub(\"RT @[A-Za-z0-9]*: \", \" \", tweet)\n",
        "\n",
        "''' This method removes all the mentions.\n",
        "    mentions are usually with @'''\n",
        "def removeMentions(tweet):\n",
        "    return re.sub(\"@[A-Za-z0-9]*\", \" \", tweet)\n",
        "\n",
        "''' This method removes multiple spaces.'''\n",
        "def removeMultipleSpaces(tweet):\n",
        "    return re.sub(\" +\", \" \", tweet)\n",
        "\n",
        "''' This method turns the tweets into lowercase. '''\n",
        "def lowercasetweet(tweet):\n",
        "    return tweet.lower()\n",
        "\n",
        "''' This method removes all the punctuations from the tweet.'''\n",
        "def removePunctuations(tweet):\n",
        "    return re.sub(\"[.,!'\\\";:?…]+\", \" \", tweet)\n",
        "\n",
        "''' This method removes special characters from tweets.'''\n",
        "def removeSpecialCharacters(tweet):\n",
        "    return re.sub(\"[@#$%^*(){}\\\\\\<>\\[\\]~/|=\\+\\-&_¿ߒ]+\",\" \", tweet)\n",
        "\n",
        "''' This method removes alpha-numeric charcters from the tweet.'''\n",
        "def removeAlphaNumeric(tweet):\n",
        "    # return re.sub(\"[A-Za-z]+[0-9]+\", \"\", tweet)\n",
        "    return re.sub(\"[0-9]+\", \"\", tweet)\n",
        "\n",
        "''' Lemmatization using nltk. '''\n",
        "def lemmatizeTweet(tweet):\n",
        "    return [WordNetLemmatizer().lemmatize(token) for token in word_tokenize(tweet)]\n",
        "\n",
        "def cleanData(text, lowercase = False, remove_stops = False, stemming = False, lemmatization = False):\n",
        "    txt = str(text)\n",
        "\n",
        "    # Replace apostrophes with standard lexicons\n",
        "    txt = txt.replace(\"isn't\", \"is not\")\n",
        "    txt = txt.replace(\"aren't\", \"are not\")\n",
        "    txt = txt.replace(\"ain't\", \"am not\")\n",
        "    txt = txt.replace(\"won't\", \"will not\")\n",
        "    txt = txt.replace(\"didn't\", \"did not\")\n",
        "    txt = txt.replace(\"shan't\", \"shall not\")\n",
        "    txt = txt.replace(\"haven't\", \"have not\")\n",
        "    txt = txt.replace(\"hadn't\", \"had not\")\n",
        "    txt = txt.replace(\"hasn't\", \"has not\")\n",
        "    txt = txt.replace(\"don't\", \"do not\")\n",
        "    txt = txt.replace(\"wasn't\", \"was not\")\n",
        "    txt = txt.replace(\"weren't\", \"were not\")\n",
        "    txt = txt.replace(\"doesn't\", \"does not\")\n",
        "    txt = txt.replace(\"'s\", \" is\")\n",
        "    txt = txt.replace(\"'re\", \" are\")\n",
        "    txt = txt.replace(\"'m\", \" am\")\n",
        "    txt = txt.replace(\"'d\", \" would\")\n",
        "    txt = txt.replace(\"'ll\", \" will\")\n",
        "\n",
        "    # Emoji replacement\n",
        "    txt = re.sub(r':\\)',r' happy ',txt)\n",
        "    txt = re.sub(r':D',r' happy ',txt)\n",
        "    txt = re.sub(r':P',r' happy ',txt)\n",
        "    txt = re.sub(r':\\(',r' sad ',txt)\n",
        "\n",
        "    # Replace words like sooooooo with so\n",
        "    txt = ''.join(''.join(s)[:2] for _, s in itertools.groupby(txt))\n",
        "    return txt\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w0K9Iks5wUj_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pickle\n",
        "pickle_in = open(\"/content/gdrive/My Drive/IR Assignment/inverted_index.pickle\",\"rb\")\n",
        "ii = pickle.load(pickle_in)\n",
        "pickle_in = open(\"/content/gdrive/My Drive/IR Assignment/count_word.pickle\",\"rb\")\n",
        "cw = pickle.load(pickle_in)\n",
        "pickle_in = open(\"/content/gdrive/My Drive/IR Assignment/count_per_document.pickle\",\"rb\")\n",
        "cd = pickle.load(pickle_in)\n",
        "pickle_in = open(\"/content/gdrive/My Drive/IR Assignment/similarity_score.pickle\",\"rb\")\n",
        "ss = pickle.load(pickle_in)\n",
        "documents = np.load('/content/gdrive/My Drive/IR Assignment/IR_assignment.npy',allow_pickle=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QIoYhnEzvaf3",
        "colab_type": "code",
        "outputId": "d18a29a1-f084-47ea-b441-003de2d576dd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "search = input(\"Enter search query:\")\n",
        "query = []\n",
        "ll = lemmatizeTweet(removeMultipleSpaces(removeURLs(removeMentions(removeEmojis(removeSpecialCharacters(removePunctuations(removeAlphaNumeric(cleanData(removeLineBreaks(search.lower()))))))))))\n",
        "for word in ll:\n",
        "  if word not in stops:\n",
        "    query.append(word.lower())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Enter search query:The Dog is dead\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iSWWWDNl7Bcn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "doc_score={}\n",
        "for line in documents:\n",
        "  doc_score[line[0]]=[]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N3a7bsHEv53Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for word in query:\n",
        "  for similar_word in ss[word]:\n",
        "    for docs in ii[similar_word[0]]:\n",
        "      score = docs.split(':')\n",
        "      doc_score[score[0]].append([-1 * similar_word[1] * log(cd[score[0]][similar_word[0]] / cw[similar_word[0]]),word,score[1]])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "caU4Dr7v7Gtg",
        "colab_type": "code",
        "outputId": "89d35240-deb5-4867-c03d-6e22a64dd40b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(doc_score['5161'])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0.8459171328833578, 'dead', '32']]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}